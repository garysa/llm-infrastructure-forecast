<h1 id="llm-infrastructure-forecast">LLM Infrastructure Forecast</h1>
<h2 id="what-is-a-tpu">1. What is a TPU?</h2>
<p>A <strong>TPU (Tensor Processing Unit)</strong> is a custom-designed
AI accelerator chip developed by Google specifically for machine
learning workloads.</p>
<h3 id="key-characteristics">Key Characteristics</h3>
<ul>
<li><strong>Purpose-built for ML</strong>: Optimized for matrix
operations and tensor computations common in neural networks</li>
<li><strong>High throughput</strong>: Excels at large-scale,
low-precision (8-bit) matrix multiplications</li>
<li><strong>Architecture</strong>: Uses a systolic array design that
efficiently moves data through processing elements</li>
<li><strong>Power efficient</strong>: Delivers more ML performance per
watt compared to general-purpose GPUs/CPUs</li>
</ul>
<h3 id="tpu-vs-gpu">TPU vs GPU</h3>
<table>
<colgroup>
<col style="width: 44%" />
<col style="width: 27%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>TPU</th>
<th>GPU</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Specialization</td>
<td>ML-only</td>
<td>General-purpose parallel computing</td>
</tr>
<tr class="even">
<td>Best for</td>
<td>Large transformers, inference at scale</td>
<td>Varied workloads, smaller models</td>
</tr>
<tr class="odd">
<td>Flexibility</td>
<td>Limited</td>
<td>High</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="will-asics-dominate-if-llms-become-mainstream">2. Will ASICs
Dominate if LLMs Become Mainstream?</h2>
<h3 id="arguments-for-asic-dominance">Arguments For ASIC Dominance</h3>
<ul>
<li><strong>Efficiency</strong>: ASICs can be 10-100x more
power-efficient than GPUs for specific workloads</li>
<li><strong>Cost at scale</strong>: Once designed, per-unit costs drop
significantly in high volume</li>
<li><strong>Inference dominance</strong>: If LLMs become ubiquitous,
inference will be the bulk of compute—ideal for ASICs</li>
<li><strong>Edge deployment</strong>: Running models on phones/devices
almost certainly requires custom silicon</li>
</ul>
<h3 id="arguments-against-gpu-resilience">Arguments Against (GPU
Resilience)</h3>
<ul>
<li><strong>Rapid model evolution</strong>: LLM architectures are still
changing fast (attention variants, MoE, SSMs like Mamba). ASICs take 2-3
years to design—risky if architectures shift</li>
<li><strong>NVIDIA’s moat</strong>: CUDA ecosystem, software stack, and
developer familiarity are deeply entrenched</li>
<li><strong>Flexibility</strong>: GPUs can run any model; ASICs may
become obsolete if paradigms change</li>
<li><strong>Hybrid approaches</strong>: NVIDIA is increasingly adding
specialized tensor cores—blurring the line</li>
</ul>
<h3 id="current-trajectory">Current Trajectory</h3>
<ul>
<li>Hyperscalers (Google, Amazon, Microsoft) are building custom chips
(TPU, Trainium, Maia)</li>
<li>Startups (Groq, Cerebras, SambaNova) are betting on specialized
architectures</li>
<li>NVIDIA still dominates (~80%+ of AI training market)</li>
</ul>
<h3 id="likely-outcome">Likely Outcome</h3>
<p>A mixed ecosystem—ASICs for inference at scale and edge, GPUs for
training and flexibility. If architectures stabilize, ASICs gain ground.
If innovation continues rapidly, GPUs remain essential.</p>
<hr />
<h2 id="enterprise-llm-deployment-hybrid-model">3. Enterprise LLM
Deployment: Hybrid Model</h2>
<p>The likely future is a <strong>hybrid model</strong> where companies
run private small LLMs for routine tasks and use public cloud LLMs for
heavy compute. This mirrors how companies handle compute generally
(on-prem + cloud).</p>
<h3 id="why-private-small-llms-make-sense">Why Private Small LLMs Make
Sense</h3>
<ul>
<li><strong>Data privacy</strong>: Sensitive data (legal, medical,
financial, IP) never leaves the network</li>
<li><strong>Latency</strong>: Local inference is faster for real-time
applications</li>
<li><strong>Cost predictability</strong>: Fixed infrastructure
vs. per-token API costs that can spike</li>
<li><strong>Customization</strong>: Fine-tuned on proprietary data,
jargon, workflows</li>
<li><strong>Compliance</strong>: Easier to meet regulatory requirements
(GDPR, HIPAA, etc.)</li>
</ul>
<h3 id="why-public-llms-for-heavy-compute">Why Public LLMs for Heavy
Compute</h3>
<ul>
<li><strong>Frontier capabilities</strong>: Largest models require
massive infrastructure</li>
<li><strong>Occasional use</strong>: Doesn’t justify owning the
hardware</li>
<li><strong>Rapid improvement</strong>: API access means instant
upgrades without redeployment</li>
<li><strong>Burst capacity</strong>: Handle spikes without
over-provisioning</li>
</ul>
<h3 id="emerging-deployment-patterns">Emerging Deployment Patterns</h3>
<table>
<thead>
<tr class="header">
<th>Use Case</th>
<th>Likely Solution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Internal chatbots, code assist</td>
<td>Private small LLM (7B-70B)</td>
</tr>
<tr class="even">
<td>Document search/RAG</td>
<td>Private, fine-tuned</td>
</tr>
<tr class="odd">
<td>Complex reasoning, research</td>
<td>Public frontier API</td>
</tr>
<tr class="even">
<td>Customer-facing products</td>
<td>Hybrid or public</td>
</tr>
<tr class="odd">
<td>Edge/embedded</td>
<td>Tiny private models (&lt;3B)</td>
</tr>
</tbody>
</table>
<h3 id="the-analogy">The Analogy</h3>
<p>It’s like databases—companies run private databases for core
operations but use cloud services for analytics, burst workloads, or
specialized capabilities.</p>
<hr />
<h2 id="the-bitcoin-asic-analogy-will-history-repeat">4. The Bitcoin
ASIC Analogy: Will History Repeat?</h2>
<p>Bitcoin mining evolved from CPUs → GPUs → FPGAs → ASICs, with ASICs
now dominating completely. Will LLM inference follow the same path?</p>
<h3 id="why-bitcoin-asics-dominated-completely">Why Bitcoin ASICs
Dominated Completely</h3>
<ul>
<li><strong>Single, fixed algorithm</strong>: SHA-256 never changes</li>
<li><strong>Pure economics</strong>: Only metric is hashes per watt per
dollar</li>
<li><strong>No flexibility needed</strong>: The workload is 100%
predictable forever</li>
<li><strong>Winner-take-all</strong>: Efficiency directly equals
profit</li>
</ul>
<h3 id="why-llm-asics-wont-dominate-as-completely">Why LLM ASICs Won’t
Dominate as Completely</h3>
<table>
<colgroup>
<col style="width: 34%" />
<col style="width: 39%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr class="header">
<th>Factor</th>
<th>Bitcoin</th>
<th>LLMs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Algorithm stability</td>
<td>Fixed forever</td>
<td>Evolving (attention → MoE → SSM?)</td>
</tr>
<tr class="even">
<td>Workload variety</td>
<td>One operation</td>
<td>Many (different models, quantizations, batch sizes)</td>
</tr>
<tr class="odd">
<td>Market maturity</td>
<td>15+ years</td>
<td>~3 years</td>
</tr>
<tr class="even">
<td>Upgrade cycle</td>
<td>Rare algorithm changes</td>
<td>New architectures yearly</td>
</tr>
</tbody>
</table>
<h3 id="where-llm-asics-will-likely-dominate">Where LLM ASICs Will
Likely Dominate</h3>
<ul>
<li><strong>Edge devices</strong> (phones, cars, IoT): ASICs will
dominate—battery life is critical</li>
<li><strong>High-volume inference</strong>: Running the same 7B model
billions of times justifies custom silicon</li>
<li><strong>Commoditized models</strong>: Once a model becomes “good
enough” and stable (like Llama-class), ASICs become viable</li>
</ul>
<h3 id="likely-pattern-by-use-case">Likely Pattern by Use Case</h3>
<table>
<thead>
<tr class="header">
<th>Use Case</th>
<th>Dominant Hardware</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Training</td>
<td>GPUs (too dynamic)</td>
</tr>
<tr class="even">
<td>Large inference (cloud)</td>
<td>Mix of GPUs + specialized accelerators</td>
</tr>
<tr class="odd">
<td>Small inference (edge)</td>
<td>ASICs (similar to Bitcoin)</td>
</tr>
</tbody>
</table>
<h3 id="the-key-variable">The Key Variable</h3>
<p>Architecture stability determines ASIC viability. If transformers
remain the standard for 5+ years, ASICs will take over inference. If
major shifts occur (like Mamba/SSMs gaining traction), GPU flexibility
remains valuable.</p>
<hr />
<h2
id="the-fragmented-agi-future-data-sovereignty-forces-decentralization">5.
The Fragmented AGI Future: Data Sovereignty Forces Decentralization</h2>
<p>The current centralized API model (everyone sends data to
OpenAI/Anthropic/Google) is unlikely to survive the path to AGI. Data
security requirements in a capitalist model will force
fragmentation.</p>
<h3 id="why-centralized-apis-wont-scale-to-agi">Why Centralized APIs
Won’t Scale to AGI</h3>
<ul>
<li><strong>Data is the moat</strong>: Corporations won’t send
proprietary data to potential competitors</li>
<li><strong>Regulatory pressure</strong>: GDPR, HIPAA, national security
laws prohibit cross-border data flows</li>
<li><strong>Competitive risk</strong>: Training data leakage could
destroy competitive advantage</li>
<li><strong>National security</strong>: Governments won’t route
sensitive queries through foreign systems</li>
</ul>
<h3 id="the-emerging-tiered-model">The Emerging Tiered Model</h3>
<figure>
<img src="agi_future_tiers.png" alt="AGI Future Tiers" />
<figcaption aria-hidden="true">AGI Future Tiers</figcaption>
</figure>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 18%" />
<col style="width: 31%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr class="header">
<th>Tier</th>
<th>Users</th>
<th>Model Type</th>
<th>Data Policy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Tier 1: Public/Open</strong></td>
<td>Education, researchers, general public</td>
<td>Open source (Llama, Mistral)</td>
<td>Public data only</td>
</tr>
<tr class="even">
<td><strong>Tier 2: Enterprise</strong></td>
<td>Corporations</td>
<td>Private fine-tuned, on-prem</td>
<td>Data stays internal</td>
</tr>
<tr class="odd">
<td><strong>Tier 3: Regulated</strong></td>
<td>Healthcare, finance, legal</td>
<td>Certified &amp; audited</td>
<td>Compliance-first</td>
</tr>
<tr class="even">
<td><strong>Tier 4: Sovereign</strong></td>
<td>Governments, defense, intelligence</td>
<td>Air-gapped, national</td>
<td>Complete isolation</td>
</tr>
</tbody>
</table>
<h3 id="market-projection">Market Projection</h3>
<figure>
<img src="agi_market_projection.png" alt="Market Projection" />
<figcaption aria-hidden="true">Market Projection</figcaption>
</figure>
<p>The centralized API model (currently ~85% of AI compute market) will
decline to ~10% by 2032 as: - Enterprise moves compute on-premises -
Governments mandate sovereign AI capabilities - Open models become
capable enough for public use</p>
<h3 id="the-google-analogy">The Google Analogy</h3>
<p>Just as Google Search is “free” for public use while enterprises pay
for private search appliances and governments build classified systems,
AGI will fragment into:</p>
<ul>
<li><strong>Public AGI</strong>: Ad-supported or government-subsidized
for education/general use</li>
<li><strong>Enterprise AGI</strong>: Licensed, on-prem, fine-tuned on
proprietary data</li>
<li><strong>Sovereign AGI</strong>: National AI capabilities, completely
isolated</li>
</ul>
<h3 id="implications">Implications</h3>
<ol type="1">
<li><strong>No single AGI monopoly</strong>: Unlike search (Google
dominance), AGI will be fragmented by design</li>
<li><strong>NVIDIA benefits</strong>: Sells hardware to all tiers, not
dependent on any single provider</li>
<li><strong>Open source critical</strong>: Public tier depends on open
models (Llama successors)</li>
<li><strong>Talent fragmentation</strong>: AI researchers spread across
government, enterprise, public sectors</li>
</ol>
<hr />
<h2 id="the-power-bottleneck-does-china-win-the-7-year-race">6. The
Power Bottleneck: Does China Win the 7-Year Race?</h2>
<p>If power becomes the primary constraint on AI scaling, geopolitical
dynamics shift dramatically. China’s infrastructure advantages could
prove decisive.</p>
<h3 id="the-power-problem">The Power Problem</h3>
<ul>
<li><strong>Current AI data center</strong>: 50-100 MW typical</li>
<li><strong>Next-gen training clusters</strong>: 500 MW - 1 GW
required</li>
<li><strong>GPT-5 class training run</strong>: Estimated 100+ MW
sustained for months</li>
<li><strong>AGI-scale compute</strong>: Potentially 5-10 GW dedicated
facilities</li>
</ul>
<p>For context: 1 GW = roughly one nuclear reactor’s output</p>
<h3 id="why-power-is-the-bottleneck">Why Power is the Bottleneck</h3>
<table>
<colgroup>
<col style="width: 60%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="header">
<th>Constraint</th>
<th>Impact</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Grid capacity</td>
<td>Most grids can’t deliver GW-scale to single sites</td>
</tr>
<tr class="even">
<td>Permitting</td>
<td>New power plants take 5-10 years in the West</td>
</tr>
<tr class="odd">
<td>Transmission</td>
<td>Building new high-voltage lines faces NIMBY resistance</td>
</tr>
<tr class="even">
<td>Renewable intermittency</td>
<td>AI training needs 24/7 baseload, not variable solar/wind</td>
</tr>
</tbody>
</table>
<h3 id="chinas-structural-advantages">China’s Structural Advantages</h3>
<table>
<thead>
<tr class="header">
<th>Factor</th>
<th>China</th>
<th>US/West</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Permitting speed</td>
<td>Months</td>
<td>5-10 years</td>
</tr>
<tr class="even">
<td>State coordination</td>
<td>Central planning</td>
<td>Fragmented jurisdictions</td>
</tr>
<tr class="odd">
<td>Grid buildout</td>
<td>Rapid expansion</td>
<td>Aging infrastructure</td>
</tr>
<tr class="even">
<td>Nuclear expansion</td>
<td>150+ reactors planned</td>
<td>Regulatory paralysis</td>
</tr>
<tr class="odd">
<td>Coal availability</td>
<td>Abundant (less clean)</td>
<td>Politically constrained</td>
</tr>
<tr class="even">
<td>Land acquisition</td>
<td>State-controlled</td>
<td>Private property rights</td>
</tr>
</tbody>
</table>
<h3 id="year-evolution-scenario-2026-2033">7-Year Evolution Scenario
(2026-2033)</h3>
<p><strong>Year 1-2 (2026-2027):</strong> - US leads on model
architecture and software - Power constraints begin limiting US scaling
- China breaks ground on dedicated AI power plants</p>
<p><strong>Year 3-4 (2028-2029):</strong> - US data centers hitting grid
limits - China’s new nuclear/coal plants coming online - Compute parity
approaches despite US chip lead</p>
<p><strong>Year 5-7 (2030-2033):</strong> - China achieves raw compute
advantage through power availability - US forced into efficiency-focused
approach - Winner determined by whether algorithms or compute matter
more</p>
<h3 id="the-critical-question">The Critical Question</h3>
<p><strong>If scaling laws hold</strong> (more compute = better AI):
China wins through brute force power advantage</p>
<p><strong>If algorithmic breakthroughs dominate</strong>: US/West wins
through talent and research ecosystem</p>
<h3 id="counterarguments-why-us-could-still-win">Counterarguments (Why
US Could Still Win)</h3>
<ul>
<li><strong>Chip restrictions</strong>: China still behind on
cutting-edge chips (NVIDIA H100/B100 banned)</li>
<li><strong>Talent</strong>: Top AI researchers still concentrated in
US</li>
<li><strong>Efficiency gains</strong>: US forced to innovate on
efficiency (smaller, better models)</li>
<li><strong>Allies</strong>: Japan, Taiwan, Korea, EU add to Western
compute pool</li>
<li><strong>Private capital</strong>: US tech giants can outspend
Chinese state in some scenarios</li>
</ul>
<h3 id="likely-outcome-1">Likely Outcome</h3>
<p>A bifurcated AI world by 2033: - <strong>Chinese AI sphere</strong>:
Raw power advantage, state-controlled, closed ecosystem -
<strong>Western AI sphere</strong>: Efficiency-focused, distributed,
allied nations pooling resources</p>
<p>Neither achieves global AGI monopoly. The “winner” depends on which
approach proves more effective—and we won’t know until it happens.</p>
<hr />
<p><em>Generated: February 2026</em></p>
